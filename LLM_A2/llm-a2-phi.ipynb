{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers\n!pip install -U bitsandbytes\n!pip install accelerate\n!pip install together","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nfrom together import Together\nuser_secrets = UserSecretsClient()\nyour_token = user_secrets.get_secret(\"HuggingFaceSecret\")\ntokenizer_gemma = transformers.AutoTokenizer.from_pretrained(\"google/gemma-2b-it\",use_auth_token=your_token)\n# model_gemma = transformers.AutoModelForCausalLM.from_pretrained(\n#     \"google/gemma-2b-it\",\n#     torch_dtype=torch.bfloat16,\n#     use_auth_token=your_token,\n#     load_in_4bit=True\n# )\n\n# gen_config_gemma = transformers.GenerationConfig.from_pretrained(\n#     \"google/gemma-2b-it\"\n# )\n# gen_config_gemma.max_new_tokens = 500\n# gen_config_gemma.temperature = 1e-10\n \n# pipeline_gemma = transformers.pipeline(\n#     task=\"text-generation\",\n#     model=model_gemma,\n#     tokenizer=tokenizer_gemma,\n#     return_full_text=True,\n#     generation_config=gen_config_gemma,\n#     device_map='auto',\n#     repetition_penalty=1.1,\n#     eos_token_id=tokenizer_gemma.eos_token_id,\n#     pad_token_id=tokenizer_gemma.pad_token_id\n# ) \n# torch.random.manual_seed(0)\n\n\nmodel_microsoft = transformers.AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\", \n    device_map=\"cuda\", \n    torch_dtype=torch.bfloat16,\n    load_in_4bit=True,\n    trust_remote_code=True, \n    use_auth_token=your_token,\n    \n)\ntokenizer_microsoft = transformers.AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\ngen_config_microsoft = transformers.GenerationConfig.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\"\n)\ngen_config_microsoft.max_new_tokens = 400\n \npipeline_microsoft = transformers.pipeline(\n    task=\"text-generation\",\n    model=model_microsoft,\n    tokenizer=tokenizer_microsoft,\n    generation_config=gen_config_microsoft,\n    device_map='cuda',\n    eos_token_id=tokenizer_microsoft.eos_token_id,\n    pad_token_id=tokenizer_microsoft.pad_token_id\n)        \n\n# client = Together(api_key=\"e52e5c120584a8119f7e30c8e2141f14310f9830af01ba05a06bdee51faa93c8\")\n# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# tokenizer_llama = transformers.AutoTokenizer.from_pretrained(model_id,use_auth_token=your_token)\n# model_llama = transformers.AutoModelForCausalLM.from_pretrained(\n#     model_id,\n#     use_auth_token=your_token,\n#     device_map=\"cuda\",\n#     load_in_4bit=True\n# )\n\n# gen_config_llama = transformers.GenerationConfig.from_pretrained(\n#     model_id\n# )\n# gen_config_llama.max_new_tokens = 500\n# gen_config_llama.temperature = 1e-10\n \n# pipeline_llama = transformers.pipeline(\n#     task=\"text-generation\",\n#     model=model_llama,\n#     tokenizer=tokenizer_llama,\n#     return_full_text=True,\n#     generation_config=gen_config_llama,\n#     device_map='auto',\n#     model_kwargs={\n#         'load_in_4bit': True,\n#         'bnb_4bit_quant_type': 'nf4',\n#         'bnb_4bit_use_double_quant': True,\n#         'bnb_4bit_compute_dtype': torch.bfloat16,\n#     },\n#     repetition_penalty=1.1,\n#     eos_token_id=tokenizer_llama.eos_token_id,\n#     pad_token_id=tokenizer_llama.pad_token_id\n# )        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain-community\n!pip install langchain_huggingface\n!pip install datasets\n!pip install Levenshtein","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def zero_shot_pipeline(pipeline, prompt):\n    tokenizer=pipeline.tokenizer\n    start_time = time.time()  # Start time\n\n   \n\n    generated_text = pipeline(prompt)\n\n    \n    end_time = time.time()  # End time\n    time_taken = end_time - start_time\n    output_texts=[output['generated_text'] for output in generated_text]\n    formatted_output=\"\".join(output_texts)\n    return formatted_output, time_taken\n\n# Function for Chain-of-Thought Prompting\ndef chain_of_thought_pipeline(pipeline, prompt):\n    tokenizer=pipeline.tokenizer\n    \n    \n    start_time = time.time()  # Start time\n\n    \n    generated_text = pipeline(prompt)\n\n    end_time = time.time()  # End time\n    time_taken = end_time - start_time\n\n    output_texts=[output['generated_text'] for output in generated_text]\n    formatted_output=\"\".join(output_texts)\n    return formatted_output, time_taken\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import hub\nprompt_react = hub.pull(\"hwchase17/react\")\nprint(prompt_react.template)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom datasets import load_dataset\n\n# Load the MMLU test dataset from Hugging Face\ndataset = load_dataset(\"cais/mmlu\", 'college_mathematics',split=\"test\")\n\nquestion_answer_dict = {row['question']: row['choices'] for row in dataset}\n\n# Display the first few entries of the dictionary to confirm\nfirst_entries = dict(list(question_answer_dict.items())[:5])\nprint(first_entries)\n\nanswers=[row['answer'] for row in dataset]\nprint(len(answers))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n# def parse_output(text):\n#     match = re.search(r'Answer\\s*::\\s*(\\d)', text)\n    \n#     if match:\n#         return match.group(1)  # Extract the first capturing group (the digit)\n#     else:\n#         return None\n    \ndef parse_output(text):\n    # Regex pattern to match variations like 'answer is _', 'answer: _', or 'final answer:_'\n    pattern = r'(?i)(?:answer\\s*is|answer\\s*:|final\\s*answer\\s*:|answer\\s*:\\s*option|final\\s*answer\\s*:\\s*option|final\\s*answer\\s*is\\s*:?\\s*\\\\boxed\\{|\\boxed\\{|answer\\s*:\\s*\\{)(\\d)'\n    match = re.search(pattern, text)\n    \n    if match:\n        return match.group(1)  # Extract the matched digit\n    else:\n        return None  # Return None if no match is found\n\n    \ndef calculate_accuracy(outputs,answers=answers,num_questions=100):\n    num_correct=0\n    for i in range(num_questions):\n        if(outputs[i] is not None and (int(outputs[i])==int(answers[i]+1))):\n            num_correct+=1\n    print(num_correct)\n    accuracy=(num_correct/num_questions)*100\n    return accuracy\n\ndef zero_shot_prompt(question,options):\n    return f\"\"\"Please select the correct answer by responding *only* with the corresponding number (1, 2, 3, or 4). No explanation needed. Stick to given format.\n\n\n        Question: {question}\n\n        Options:\n        1) {options[0]}\n        2) {options[1]}\n        3) {options[2]}\n        4) {options[3]}\n\n        Answer :\"\"\"\n\ndef chain_of_thought_prompt(question,options):\n    return \"Choose the answer of the given question from the below options.\\n\"+question+\"\\n\"+\"{1:\"+options[0]+\"}\"+\"\\n\"+\"{2:\"+options[1]+\"}\"+\"\\n\"+\"{3:\"+options[2]+\"}\"+\"\\n\"+\"{4:\"+options[3]+\"}\"+\"\\n\"+\"Think step by step.\"+\"\\n\"+\"answer: {\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from Levenshtein import distance\ndef parse_output_ReAct(text):\n    text = str(text).strip()  # Ensure the input is a string and remove any leading/trailing whitespace\n    lower_text = text.lower()  # Convert to lowercase for case-insensitive matching\n\n    # Check for the last occurrence of each pattern using rfind()\n\n    # Check for cases like \"The final answer is: $\\boxed{3}$\" or \"The answer is\"\n    if lower_text.rfind(\"the final answer is:\") != -1 or lower_text.rfind(\"the answer is\") != -1:\n        \n        boxed_start = lower_text.rfind(\"\\\\boxed{\")\n        if boxed_start != -1:\n            digit_start = boxed_start + len(\"\\\\boxed{\")\n            digit_end = lower_text.find(\"}\", digit_start)\n            if digit_end != -1:\n                return text[digit_start:digit_end]  # Extract the digit from the original text\n\n    # Check for the last occurrence of formats like \"answer is 3\" or \"answer: 2\"\n    if lower_text.rfind(\"answer is\") != -1:\n        \n        start_pos = lower_text.rfind(\"answer is\") + len(\"answer is\")\n        return extract_digit(text[start_pos:])  # Use the original text to extract the digit\n    \n    if lower_text.rfind(\"answer:\") != -1:\n        \n        start_pos = lower_text.rfind(\"answer:\") + len(\"answer:\")\n        return extract_digit(text[start_pos:])\n\n    # Check for \"final answer: option 4\" or \"answer: option 4\"\n    if lower_text.rfind(\"final answer: option\") != -1 or lower_text.rfind(\"answer: option\") != -1:\n        \n        start_pos = lower_text.rfind(\"option\") + len(\"option\")\n        return extract_digit(text[start_pos:])\n    \n    # Check for the last occurrence of \"answer: {2:n = 1 and r = 7}\"\n    if lower_text.rfind(\"answer: {\") != -1:\n        \n        start_pos = lower_text.rfind(\"answer: {\") + len(\"answer: {\")\n        return extract_digit(text[start_pos:])\n    \n    # If no patterns match, return None\n    return None\n\ndef extract_digit(subtext):\n    # Extract the first digit found in the substring\n    for char in subtext.strip():\n        if char.isdigit():\n            return char\n    return None\ndef find_most_similar_option(options, target):\n\n    min_distance = float('inf')\n    best_match = None\n    best_index = -1\n    \n    for index, option in enumerate(options):\n        dist = distance(target, option)\n        if dist < min_distance:\n            min_distance = dist\n            best_match = option\n            best_index = index\n            \n    return best_index+1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\npipeline=pipeline_microsoft\ntime_zero=0\noutputs_zero_shot=[]\nnum_questions=0\nfor question,options in question_answer_dict.items():\n    print(\"Zero-SHOT\")\n    prompt=zero_shot_prompt(question,options)\n    Zero_shot_output,zero_shot_time=zero_shot_pipeline(pipeline,prompt)\n    option=parse_output(Zero_shot_output)\n    if(option is not None):\n        num_questions+=1\n    print(Zero_shot_output)\n    outputs_zero_shot.append(option)\n    time_zero+=zero_shot_time\n    print(\"----------------------------------------------------\")\nacc=calculate_accuracy(outputs_zero_shot,answers,num_questions)\nprint(f\"Accuracy:{acc}\")\nprint(f\"Time:{time_zero/100}\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\npipeline=pipeline_microsoft\ntime_CoT=0\noutputs_CoT=[]\nnum_questions=0\nfor question,options in question_answer_dict.items():\n    print(\"Chain of Thought\")\n    prompt=chain_of_thought_prompt(question,options)\n    CoT_output,CoT_time=chain_of_thought_pipeline(pipeline,prompt)\n    option=parse_output(CoT_output)\n    if(option is not None):\n        num_questions+=1\n    print(CoT_output)\n    outputs_CoT.append(option)\n    time_CoT+=CoT_time\n    print(\"----------------------------------------------------\")\nacc=calculate_accuracy(outputs_CoT,answers,num_questions)\nprint(f\"Accuracy:{acc}\")\nprint(f\"Time:{time_CoT/100}\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import hub\nprompt_react = hub.pull(\"hwchase17/react\")\nprint(prompt_react.template)\n\nfrom langchain.agents import load_tools\nfrom langchain.agents import AgentExecutor,create_react_agent,initialize_agent,AgentType,Tool\nfrom langchain_huggingface import HuggingFacePipeline\nfrom io import StringIO\nfrom transformers import TextIteratorStreamer\nimport sys\nimport re\nimport io\n\ndef run_and_capture(prompt, react_agent):\n    # Create a string buffer to capture the output\n    captured_output = io.StringIO()\n    \n    # Save the original stdout\n    original_stdout = sys.stdout\n    \n    try:\n        # Redirect stdout to both the buffer and the console\n        sys.stdout = captured_output\n\n        # Run the agent with the provided prompt\n        result = react_agent.run(prompt)\n        \n        # Write the captured output directly to the original stdout (console)\n        original_stdout.write(captured_output.getvalue())\n\n    finally:\n        # Reset stdout to its original state\n        sys.stdout = original_stdout\n    \n    # Get the captured verbose output\n    verbose_output = captured_output.getvalue()\n    \n    # Return both the final result and the verbose output\n    return verbose_output\n\ndef ReAct_pipeline(pipeline,prompt):\n    tokenizer=pipeline.tokenizer\n    prompt_react = hub.pull(\"hwchase17/react\")\n    \n    llm = HuggingFacePipeline(pipeline=pipeline)\n\n    tools = load_tools([\"llm-math\"], llm=llm)\n\n\n    react_agent = initialize_agent(\n        llm=llm,  # The language model pipeline\n        tools=tools, # The tools, e.g., llm-math  \n        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n        prompt=prompt_react,\n        handle_parsing_errors=True,\n        max_iterations=1,\n        verbose=True\n    )\n    start_time = time.time()  # Start time\n    output = run_and_capture(prompt,react_agent)\n    end_time = time.time()  # End time\n    time_taken = end_time - start_time\n    return output,time_taken\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline=pipeline_microsoft\ntime_ReAct=0\noutputs_ReAct=[]\nnum_questions=0\nfor question,options in question_answer_dict.items():\n    print(\"ReAct Prompting\")\n    prompt=chain_of_thought_prompt(question,options)\n    ReAct_output,ReAct_time=ReAct_pipeline(pipeline,prompt)\n    option=parse_output_ReAct(ReAct_output)\n    if(option not in ['1','2','3','4']):\n        for i in range(4):\n            if(option==options[i]):\n                option=i+1\n    if(option not in ['1','2','3','4']):\n        option=find_most_similar_option(ReAct_output,options)\n    if(option is not None):\n        num_questions+=1\n    outputs_ReAct.append(option)\n    print(option)\n    time_ReAct+=ReAct_time\n    print(\"----------------------------------------------------\")\nacc=calculate_accuracy(outputs_ReAct,answers,num_questions)\nprint(f\"Accuracy:{acc}\")\nprint(f\"Time:{time_ReAct/100}\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}